{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use reviews to predict the rating of Amazon Instant Video by CNN, GRU and LSTM model. Besides, we use NLP to manipulate reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UE7znKnLMwab"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "Ed8-7GOINT5f",
    "outputId": "8977ee6b-09c3-4771-c2f4-878154457f40",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A1POFVVXUZR3IQ</td>\n",
       "      <td>I discovered this series quite by accident. Ha...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1PG2VV4W1WRPL</td>\n",
       "      <td>It beats watching a blank screen. However, I j...</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ATASGS8HZHGIB</td>\n",
       "      <td>There are many episodes in this series, so I p...</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3RXD7Z44T9DHW</td>\n",
       "      <td>This is the best of the best comedy Stand-up. ...</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUX8EUBNTHIIU</td>\n",
       "      <td>Not bad.  Didn't know any of the comedians but...</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                                         reviewText  \\\n",
       "0  A11N155CW1UV02  I had big expectations because I love English ...   \n",
       "1  A3BC8O2KCL29V2  I highly recommend this series. It is a must f...   \n",
       "2   A60D5HQFOTSOM  This one is a real snoozer. Don't believe anyt...   \n",
       "3  A1RJPIGRSNX4PW  Mysteries are interesting.  The tension betwee...   \n",
       "4  A16XRPF40679KG  This show always is excellent, as far as briti...   \n",
       "5  A1POFVVXUZR3IQ  I discovered this series quite by accident. Ha...   \n",
       "6  A1PG2VV4W1WRPL  It beats watching a blank screen. However, I j...   \n",
       "7   ATASGS8HZHGIB  There are many episodes in this series, so I p...   \n",
       "8  A3RXD7Z44T9DHW  This is the best of the best comedy Stand-up. ...   \n",
       "9   AUX8EUBNTHIIU  Not bad.  Didn't know any of the comedians but...   \n",
       "\n",
       "         asin  overall  \n",
       "0  B000H00VBQ      2.0  \n",
       "1  B000H00VBQ      5.0  \n",
       "2  B000H00VBQ      1.0  \n",
       "3  B000H00VBQ      4.0  \n",
       "4  B000H00VBQ      5.0  \n",
       "5  B000H00VBQ      5.0  \n",
       "6  B000H0X79O      3.0  \n",
       "7  B000H0X79O      3.0  \n",
       "8  B000H0X79O      5.0  \n",
       "9  B000H0X79O      3.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_list_of_dicts(fname):\n",
    "    return [json.loads(i) for i in open(fname,'rt')]\n",
    "\n",
    "raw_data = get_list_of_dicts('Amazon_Instant_Video_5.json')\n",
    "\n",
    "data = pd.DataFrame(raw_data).loc[:, [\"reviewerID\",'reviewText', \"asin\", \"overall\"]]\n",
    "data.head(10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "KAcujYbKPe-6",
    "outputId": "8017245b-ee89-4aaf-8df1-738b4781b5c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I really like the characters and the actors. I...</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one good show. It is interesting in it...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A1POFVVXUZR3IQ</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Auschwitz: Inside the Nazi State makes for com...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1PG2VV4W1WRPL</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Very slow moving plot if there is one.  Trying...</td>\n",
       "      <td>This is the best of the best comedy Stand-up. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ATASGS8HZHGIB</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I think there are 10 episodes in this series, ...</td>\n",
       "      <td>It beats watching a blank screen. However, I j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3RXD7Z44T9DHW</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>All the comedians are hilarious. I have seen t...</td>\n",
       "      <td>It beats watching a blank screen. However, I j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUX8EUBNTHIIU</td>\n",
       "      <td>B000H0X79O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Yes, and who cares.  The imagination can conju...</td>\n",
       "      <td>It beats watching a blank screen. However, I j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "5  A1POFVVXUZR3IQ  B000H00VBQ      5.0   \n",
       "6  A1PG2VV4W1WRPL  B000H0X79O      3.0   \n",
       "7   ATASGS8HZHGIB  B000H0X79O      3.0   \n",
       "8  A3RXD7Z44T9DHW  B000H0X79O      5.0   \n",
       "9   AUX8EUBNTHIIU  B000H0X79O      3.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  I really like the characters and the actors. I...   \n",
       "1  This is one good show. It is interesting in it...   \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "5  Auschwitz: Inside the Nazi State makes for com...   \n",
       "6  Very slow moving plot if there is one.  Trying...   \n",
       "7  I think there are 10 episodes in this series, ...   \n",
       "8  All the comedians are hilarious. I have seen t...   \n",
       "9  Yes, and who cares.  The imagination can conju...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  \n",
       "5  I had big expectations because I love English ...  \n",
       "6  This is the best of the best comedy Stand-up. ...  \n",
       "7  It beats watching a blank screen. However, I j...  \n",
       "8  It beats watching a blank screen. However, I j...  \n",
       "9  It beats watching a blank screen. However, I j...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_user_reviews(x):\n",
    "    ur = user_reviews.loc[x['reviewerID']].drop(x['asin']).values.tolist()\n",
    "    mr = movie_reviews.loc[x['asin']].drop(x['reviewerID']).values.tolist()\n",
    "    x['userReviews'] = ' '.join(list(map(lambda x: x[0], ur)))\n",
    "    x['movieReviews'] = ' '.join(list(map(lambda x: x[0], mr)))\n",
    "    return x\n",
    "\n",
    "user_item_review = data.drop('reviewText', axis = 1)\n",
    "user_reviews = pd.pivot_table(data, index = ['reviewerID','asin'], aggfunc = lambda x: x).drop('overall', axis = 1)\n",
    "movie_reviews = pd.pivot_table(data, index = ['asin', 'reviewerID'], aggfunc = lambda x: x).drop('overall', axis = 1)\n",
    "\n",
    "df = user_item_review.apply(add_user_reviews, axis = 1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "bnRZTal-SeaK",
    "outputId": "aa97b33f-31e9-4a6e-9238-f2b7dd1796c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I really like the characters and the actors. I...</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one good show. It is interesting in it...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  I really like the characters and the actors. I...   \n",
       "1  This is one good show. It is interesting in it...   \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split \n",
    "test_size = 0.005\n",
    "\n",
    "# get test size percentage of users\n",
    "unique_users = df.loc[:, 'reviewerID'].unique()\n",
    "users_size = len(unique_users)\n",
    "test_idx = np.random.choice(users_size, size = int(users_size * test_size), replace = False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = df[df['reviewerID'].isin(test_users)]\n",
    "train = df[df['reviewerID'].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test['asin'].unique()\n",
    "# drop the movies that also appear in our test set. in order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train['asin'].isin(unique_test_movies))).dropna()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmKmnzrKoE-I"
   },
   "source": [
    "Embed the reviews into glove word2vect model\n",
    "\n",
    "The pre-trained model is downloadable at https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_LgKMsG_oDyY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# functions to embed user reviews into the glove word2vect model\n",
    "def init_embeddings_map(fname):\n",
    "    with open(os.path.join(\"glove.6B\", fname), encoding = 'utf8') as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype = 'float32') for l in [line.split() for line in glove]}\n",
    "\n",
    "def get_embed_func(i_len, u_len, pad_value, embedding_map):\n",
    "    def embed(row):\n",
    "        sentence = row['userReviews'].split()[:u_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word) if word in embedding_map else pad_value, sentence))\n",
    "        row['userReviews'] = reviews + [pad_value] * (u_len - len(reviews))\n",
    "        sentence = row['movieReviews'].split()[:i_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word) if word in embedding_map else pad_value, sentence))\n",
    "        row['movieReviews'] = reviews + [pad_value] * (i_len - len(reviews))\n",
    "        return row\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yyP3-RZxul40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size  = 50\n",
    "embedding_map = init_embeddings_map('glove.6B.' + str(emb_size) + 'd.txt')\n",
    "\n",
    "user_sizes = df.loc[:, 'userReviews'].apply(lambda x: x.split()).apply(len)\n",
    "item_sizes = df.loc[:, 'movieReviews'].apply(lambda x: x.split()).apply(len)\n",
    "\n",
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_len = int(np.percentile(user_sizes, u_ptile))\n",
    "i_len = int(np.percentile(item_sizes, i_ptile))\n",
    "\n",
    "embedding_fn = get_embed_func(i_len, u_len, np.array([0.0] * emb_size), embedding_map)\n",
    "\n",
    "train_embedded = train.apply(embedding_fn, axis = 1)\n",
    "test_embedded = test.apply(embedding_fn, axis = 1)\n",
    "\n",
    "print(u_len, i_len) # size of input in deep neural networks, useful to set parameter\n",
    "train_embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck_buE3GygmT"
   },
   "source": [
    "# Deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zuX8AwTmy4e8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.python.keras.layers.merge import Add, Dot, Concatenate\n",
    "from tensorflow.python.keras.layers import Conv1D ,GRU, LSTM, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VBrYjHn-0SqI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 241, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 722, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 232, 4)       2004        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 713, 4)       2004        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 116, 4)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 356, 4)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 107, 4)       164         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 347, 4)       164         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 53, 4)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 173, 4)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 212)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 692)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           13632       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           44352       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dot[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 62,449\n",
      "Trainable params: 62,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_tower(max_len, embedding_size, hidden_size, filters=4, kernel_size=10):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(tower)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def CNN_model(embedding_size, hidden_size, u_len, i_len):\n",
    "    inputU, towerU = cnn_tower(u_len, embedding_size, hidden_size)\n",
    "    inputM, towerM = cnn_tower(i_len, embedding_size, hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "model_cnn = CNN_model(emb_size, hidden_size, u_len, i_len)\n",
    "model_cnn.compile(optimizer='Adam', loss='mse')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ching\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "759/759 [==============================] - 23s 28ms/step - loss: 1.7714 - val_loss: 1.7490\n",
      "Epoch 2/20\n",
      "759/759 [==============================] - 19s 25ms/step - loss: 1.4364 - val_loss: 1.8209\n",
      "Epoch 3/20\n",
      "759/759 [==============================] - 20s 26ms/step - loss: 1.3277 - val_loss: 1.6010\n",
      "Epoch 4/20\n",
      "759/759 [==============================] - 20s 26ms/step - loss: 1.2964 - val_loss: 1.5319\n",
      "Epoch 5/20\n",
      "759/759 [==============================] - 19s 25ms/step - loss: 1.2137 - val_loss: 1.5043\n",
      "Epoch 6/20\n",
      "759/759 [==============================] - 19s 25ms/step - loss: 1.1926 - val_loss: 1.5152\n",
      "Epoch 7/20\n",
      "759/759 [==============================] - 18s 24ms/step - loss: 1.1624 - val_loss: 1.5821\n",
      "Epoch 8/20\n",
      "759/759 [==============================] - 18s 24ms/step - loss: 1.1535 - val_loss: 1.5351\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "user_reviews = np.array(list(train_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(train_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "train_inputs = [user_reviews, movie_reviews]\n",
    "train_outputs = train_embedded.loc[:, \"overall\"]\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"cnn_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('cnn_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_cnn.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_cnn.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 241, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 722, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 64)           22080       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 64)           22080       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_5[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 52,609\n",
      "Trainable params: 52,609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def gru_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = GRU(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def GRU_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = gru_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = gru_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_gru = GRU_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_gru.compile(optimizer='Adam', loss='mse')\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "759/759 [==============================] - 385s 502ms/step - loss: 1.9333 - val_loss: 1.8281\n",
      "Epoch 2/20\n",
      "759/759 [==============================] - 409s 539ms/step - loss: 1.3990 - val_loss: 1.5182\n",
      "Epoch 3/20\n",
      "759/759 [==============================] - 445s 586ms/step - loss: 1.3334 - val_loss: 1.4807\n",
      "Epoch 4/20\n",
      "759/759 [==============================] - 513s 676ms/step - loss: 1.2762 - val_loss: 1.4487\n",
      "Epoch 5/20\n",
      "759/759 [==============================] - 502s 662ms/step - loss: 1.2135 - val_loss: 1.4375\n",
      "Epoch 6/20\n",
      "759/759 [==============================] - 509s 670ms/step - loss: 1.2208 - val_loss: 1.5109\n",
      "Epoch 7/20\n",
      "759/759 [==============================] - 499s 657ms/step - loss: 1.1702 - val_loss: 1.4504\n",
      "Epoch 8/20\n",
      "759/759 [==============================] - 511s 674ms/step - loss: 1.1139 - val_loss: 1.4199\n",
      "Epoch 9/20\n",
      "759/759 [==============================] - 525s 692ms/step - loss: 1.0737 - val_loss: 1.4260\n",
      "Epoch 10/20\n",
      "759/759 [==============================] - 558s 735ms/step - loss: 1.0561 - val_loss: 1.4493\n",
      "Epoch 11/20\n",
      "759/759 [==============================] - 568s 749ms/step - loss: 1.0203 - val_loss: 1.4375\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"gru_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('gru_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_gru.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_gru.save(\"gru.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 241, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 722, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           29440       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           29440       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           4160        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           4160        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            129         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           dense_8[0][0]                    \n",
      "                                                                 dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 67,329\n",
      "Trainable params: 67,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "def lstm_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = LSTM(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def LSTM_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = lstm_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = lstm_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_lstm = LSTM_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_lstm.compile(optimizer='Adam', loss='mse')\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "759/759 [==============================] - 974s 1s/step - loss: 0.9659 - val_loss: 1.4822\n",
      "Epoch 2/30\n",
      "759/759 [==============================] - 1000s 1s/step - loss: 0.9342 - val_loss: 1.4918\n",
      "Epoch 3/30\n",
      "759/759 [==============================] - 1035s 1s/step - loss: 0.9191 - val_loss: 1.4908\n",
      "Epoch 4/30\n",
      "759/759 [==============================] - 1037s 1s/step - loss: 0.8906 - val_loss: 1.4743\n",
      "Epoch 5/30\n",
      "759/759 [==============================] - 1051s 1s/step - loss: 0.8608 - val_loss: 1.5421\n",
      "Epoch 6/30\n",
      "759/759 [==============================] - 1056s 1s/step - loss: 0.8446 - val_loss: 1.4880\n",
      "Epoch 7/30\n",
      "759/759 [==============================] - 1069s 1s/step - loss: 0.8161 - val_loss: 1.4727\n",
      "Epoch 8/30\n",
      "759/759 [==============================] - 1081s 1s/step - loss: 0.7907 - val_loss: 1.4770\n",
      "Epoch 9/30\n",
      "759/759 [==============================] - 1099s 1s/step - loss: 0.7702 - val_loss: 1.5020\n",
      "Epoch 10/30\n",
      "759/759 [==============================] - 1131s 1s/step - loss: 0.7541 - val_loss: 1.4970\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"lstm_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('lstm_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_lstm.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_lstm.save(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for CNN model : 1.5703979231145906\n",
      "Test MSE for GRU model : 1.4958523094066452\n",
      "Test MSE for LSTM model : 1.484589167566555\n"
     ]
    }
   ],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "\n",
    "predictions_cnn = model_cnn.predict(test_inputs)\n",
    "predictions_gru = model_gru.predict(test_inputs)\n",
    "predictions_lstm = model_lstm.predict(test_inputs)\n",
    "\n",
    "error_cnn = np.square(predictions_cnn - true_rating)\n",
    "print(\"Test MSE for CNN model :\", np.average(error_cnn))\n",
    "\n",
    "error_gru = np.square(predictions_gru - true_rating)\n",
    "print(\"Test MSE for GRU model :\", np.average(error_gru))\n",
    "\n",
    "error_lstm = np.square(predictions_lstm - true_rating)\n",
    "print(\"Test MSE for LSTM model :\", np.average(error_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is sparse. We need to run more epochs or collect more data.\n",
    "\n",
    "1. RNN is better than CNN, because the reviews are sequential data.\n",
    "\n",
    "2. LSTM is better than GRU."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "deep learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
